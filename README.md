# ğŸ“° newsgroup_semantics

## Manifold Learning & Semantic Discovery Pipeline

Resolving high-dimensional sparsity through optimized latent semantic analysis.

## ğŸ›ï¸ Architecture

This repository benchmarks **Classical (TF-IDF)** vs. **Embedding-based (Doc2Vec)** architectures for **unsupervised topic discovery**.

It explicitly rejects the _â€œblobâ€ artifacts_ common in naÃ¯ve dimensionality-reduction pipelines by enforcing **dense feature initialization before manifold projection**.

### ğŸ§  Latent Discovery

- Compresses **16,000+ sparse dimensions**
- Produces **50 dense semantic features**
- Preserves global structure before non-linear projection

### ğŸ“ Modular Design

- Clean separation of concerns:
  - **ETL**
  - **Vectorization**
  - **Visualization**
- Production-grade utilities isolated from exploratory notebooks

<h2>ğŸ—¿ Visual Dominance</h2>

<p><em>Baseline vs. Optimization</em></p>

<table>
  <tr>
    <td align="center">
      <strong>NaÃ¯ve t-SNE (Raw Sparse)</strong><br>
      <img src="results/figures/tsne_naive_raw_sparse.png" width="420">
      <br><em>Messy starburst artifact</em>
    </td>
    <td align="center">
      <strong>SVD-Initialized t-SNE</strong><br>
      <img src="results/figures/tsne_svd_reduced.png" width="420">
      <br><em>Clear semantic islands</em>
    </td>
  </tr>
</table>

<p><strong>Observation:</strong><br>
SVD initialization completely mogs the raw sparse t-SNE. By collapsing the TF-IDF space into a dense, low-rank semantic subspace before non-linear projection, it successfully unfolds the manifold, separating Hockey (Cyan) from Baseball (Pink) where raw Euclidean distance failed. Check notebook for more information. </p>

## ğŸ“‚ Repository Structure

```plaintext
nlp-clustering-benchmarks/
â”œâ”€â”€ data/                  # ğŸ”’ Gitignored raw assets
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_baseline.ipynb  # ğŸ· TF-IDF & SVD Optimization
â”‚   â””â”€â”€ 02_research.ipynb  # ğŸš§ Doc2Vec Experimentation (Local)
â”œâ”€â”€ src/                   # ğŸ—ï¸ Production-grade modules
â”‚   â””â”€â”€ utils.py           #    ETL & Preprocessing engine
â””â”€â”€ requirements.txt       # ğŸ’¼ Dependency lockfile
```

## ğŸš€ Quick Start

Assert dominance over your environment:

```bash
# 1. Clone the repo

git clone https://github.com/YOUR_USERNAME/nlp-clustering-benchmarks.git

# 2. Install dependencies

pip install -r requirements.txt

# 3. Execute the pipeline

jupyter notebook notebooks/01_baseline.ipynb

```
